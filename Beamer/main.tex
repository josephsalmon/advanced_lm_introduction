\documentclass[unknownkeysallowed]{beamer}
\usepackage[french,english]{babel}
\usepackage{../../sty/beamer_js}
\usepackage{../../sty/shortcuts_js}
\usepackage{csquotes}


\graphicspath{{./prebuiltimages/},{../../sharedimages/}}

\addbibresource{Bibliographie.bib}
\usepackage{enumerate}

\begin{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%             Headers               %%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\bigskip
\bigskip
\begin{center}{
\LARGE\color{marron}
\textbf{HMMA 307 : \\ Advanced Linear Modeling}
\textbf{ }\\
\vspace{0.5cm}
}

\color{marron}
\textbf{Chapter 1 : Linear regression}
\end{center}

\vspace{0.5cm}

\begin{center}
\textbf{Emma Santinelli \ Mégane Diéval \ Yassine Sayd} \\
\vspace{0.1cm}
\url{https://github.com/MegDie/advanced_lm_introduction}\\
\vspace{0.5cm}
Université de Montpellier \\
\end{center}

\centering
\includegraphics[width=0.13\textwidth]{umontpellier_logo}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%       PLAN      %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Table of Contents}
\tableofcontents[hideallsubsections]
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\AtBeginSection[]
{
\begin{frame}<beamer>{Table of Contents}
\tableofcontents[currentsubsection,
    hideothersubsections,
    sectionstyle=show/shaded,
]
\end{frame}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction and Ordinary Least Squares}
\label{sec:introdcution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Model}
Observations: $n$ samples $(y_i, x_i)_{i=1,\dots,n}$ with $p$ features.

The model can be written in matrix notation as :

\begin{align*}
	\boxed{y=X\beta+\varepsilon}
\end{align*}

where
\begin{itemize}
	\item $X=[\bfx_1,\dots,\bfx_p]= [x_1^\top,\dots,x_n^\top]^\top$ is an $n \times p$ matrix of covariates/features
	\item $\beta$ is a $p \times 1$ vector of unknown parameters
	\item $\varepsilon$ is a vector of \iid random normal errors with mean $0$
\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{(Ordinary) Least squares:  $\hat\beta^{\rm LS}$}
The \mybold{LS} estimator is any coefficient vector
$\hat\beta^{\rm LS}\in\mathbb{R}^p$ such that :
\newline
\begin{align*}
\hat\beta^{\rm LS} \in \argmin_{\beta \in \bbR^p}
\underbrace{\frac{1}{2n}\|y-X\beta\|^2}_{f(\beta)}
\end{align*}
\vspace{0.25cm}
and
\begin{align*}
	f(\beta)
	= \frac{1}{2n}\sum_{i=1}^n (y_{i}-\frac{1}{2n}(X\beta)_{i})^2
	= \beta^{\top}\frac{X^{\top}X}{2n}\beta+\frac{1}{2n}\|y\|^2- \langle y,X\beta\rangle
\end{align*}

\vspace{0.25cm}

where
$\langle y,X\beta\rangle=y^{\top}X\beta=\beta^{\top}X^{\top}y=\langle \beta,X^{\top}y\rangle$

\vspace{0.5cm}

\rem $1/2$ is convenient for optimization (computing gradients), and $1/n$ for if $n\to\infty$ ($p$ fixed) then the objective function convergences to something like $\bbE\left[\frac{1}{2n} (y_{\infty}-x^\top_{\infty} \beta\right]^2$.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Gram Matrix}
\mytheorem{Notation}{
The matrix $\hat\Sigma=\frac{X^{\top}X}{n}$ is called the \mybold{Gram} matrix.

\begin{align*}
	X^{\top}X=\begin{pmatrix}
   \bfx_{1}^{\top}  \\
   \vdots   \\
   \bfx_{p}^{\top}  \\
\end{pmatrix}(\bfx_{1},\dots, \bfx_{p})
\end{align*}
}

\vspace{1cm}
Elementwise Gram matrix :
$[X^{\top}X]_{j,j'}
=
[\langle \bfx_{j}, \bfx_{j'}\rangle]_{(j,j') \in \llbracket 1,p \rrbracket^2}$


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Standarization: centering} % XXX TODO give a name

\mybold{Feature centering}\footnote{the average is performed along samples!}:

Compute the columns sample means:

\begin{align}\label{eq:mean}\tag{*}
 \bar{\bfx}_{j} = \frac{1}{n} \sum_{i=1}^{n} x_{ij}
\end{align}

Use Equation~\eqref{eq:mean} to get the centered matrix:

\begin{align*}
	{\rm centering} (X) := \bar{X} = X - \left[\bar{\bfx}_{1}\ind_n,\dots,\bar{\bfx}_{p}\ind_n\right]
\end{align*}

where $\ind_n=(1,\dots,1)^\top$

\vspace{0.4cm}

\rem $\bar{X}$ has columns with zero means

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Standardization: scaling}

\mybold{Feature scaling} (reduction):
\begin{align*}
	\hat\sigma_{j}^2 = \frac{1}{n} \sum_{i=1}^{n} (x_{ij}-\bar{\bfx}_{j})^2
\end{align*}


and then one can define:

\begin{align*}
	S_X = \diag(\hat\sigma_{1},\dots, \hat\sigma_{p})
\end{align*}


To get the standardized matrix:

\begin{align*}
	{\rm stdzing}(X)
	& = {\rm center}(X) \cdot S_X^{-1} \\
	& = \left[\frac{\bfx_{a}-\bar{\bfx}_{a}\ind_n}{\hat\sigma_{a}},
			  \dots,
   	          \frac{\bfx_{p}-\bar{\bfx}_{p}\ind_n}{\hat\sigma_{p}}
        \right]
\end{align*}

\rem  \texttt{sklearn}\footnote{\url{https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html}} convention is $1/n$ (could have been $1/(n-1)$)

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Optimization}
\mytheorem{First Order Optimality Conditions}{
Since $f$ is differentiable over $\bbR^p$, the following holds:

\begin{align*}
	\nabla{f(\hat\beta^{\rm LS})}=0
\end{align*}
}

\vspace{0.2cm}

\rem If $f$ is even $C^{\infty}$ a function

\vspace{0.2cm}

\rem $f$ is a convex function so a local minimum is a global one

\vspace{0.2cm}

\underline{Conclusion}:
$\hat\beta^{\rm LS}$ satisfies the following equations of orthogonality :

\begin{align*}
	\nabla{f(\hat\beta^{\rm LS})} = 0
	& \iff\frac{X^{\top}X}{n}\hat\beta^{\rm LS}-\frac{X^{\top}y}{n}=0\\
	& \iff X^{\top}(\frac{X\hat\beta^{\rm LS}-y}{n})=0\\
	& \iff X^{\top}(y-X\hat\beta^{\rm LS})=0\\
	& \iff \langle \bfx_{j},y-X\beta\rangle=0, \forall j \in \llbracket 1,p\rrbracket\\
\end{align*}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{High dimension warning} % XXX TODO give a name


\danger When $p  > n$ \textbf{and}  ${\rm rank}(X) \leq n $, then, $\hat{\beta}^{\rm LS}$ is not unique

\vspace{1cm}

\rem this happens when $\hat\Sigma$ is singular

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Normal equations}
\mytheorem{Interpretation}{
Each feature is orthogonal to the residuals $r=y-X\hat{\beta}^{\rm LS}$:

\begin{align*}
	\forall j \in \llbracket 1,p\rrbracket, \langle r,\bfx_j\rangle = 0
\end{align*}


The LS vector $\hat{\beta}^{\rm LS}$ is a solution of a  $p \times p$ linear system $\beta$:

\vspace{-0.20cm}

\begin{align*}
\hat{\Sigma}\beta= \frac{X^{\top} y}{n}
\end{align*}
\vspace{-0.80cm}

}

\rem
    \begin{itemize}
        \item  $\hat{\Sigma}$ is invertible $\Rightarrow$ the solution of the linear system is unique
        \item $\hat{\Sigma}$ is invertible $\Rightarrow$ $\hat{\Sigma}$ is positive definite
        \item $\hat{\Sigma}$ invertible $\Rightarrow$ ${\rm rank}(\hat{\Sigma})=p$
        \item we assume that we have a full rank column e.g. : $${\rm rank}(X)=\dim(\vect(X_1,\dots,X_p)) \leq n $$
    \end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{The full column rank case} % XXX TODO give a name

\mytheorem{Theorem}{
If ${\rm rank}(X)=p$, then $\hat{\Sigma}$ is invertible and one has

\begin{align*}
	\hat{\beta}^{\rm LS}=(X^{\top}X)^{-1}X^{\top}y
\end{align*}
}
\vspace{0.6cm}

Proof:
 $$\hat{\beta}^{\rm LS}=\hat{\Sigma}^{-1}\frac{X^{\top} y}{n}=\left(\frac{X^{\top} X}{n}\right)^{-1} \frac{X^{\top} y}{n}$$

\vspace{0.3cm}

\rem In practice you hardly ever invert $\hat{\Sigma} $, but rather solve a linear system (inverting = solving $p$ systems here, when one is enough)
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Data analysis}
\begin{exampleblock}{Motivation}
Using ordinary least squares models on two datasets:

\begin{itemize}
	\item Bicycle accidents
	\item Count data of bicycles
	\end{itemize}
\end{exampleblock}

\vspace{0.5cm}

We propose to estimate the severity of accidents by the feature "sexe". The problem is that the features are qualitative:
\begin{itemize}
    \item Modalities of the feature to predict: "0 - Indemne", "1 - Blessé léger", "Blessé hospitalisé", and "3 - Tué"
    \item Modalities of the feature "sexe": "M" and "F"
\end{itemize}
Source: see associated code % XXX TODO: details better the dataset
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Data analysis}

First: convert features into ordinal features.

\begin{exampleblock}{Prediction principle}

Calculate the coefficients $\beta$ on a training sample and predict on a test sample the feature of interest. 0 is the value for male and 1 is the value for female.

\end{exampleblock}


\begin{minipage}[c]{.36\linewidth}
     \begin{center}
             \includegraphics[width=5.5cm]{stat_model_gravity}
         \end{center}
   \end{minipage} \hfill
   \begin{minipage}[c]{.55\linewidth}
    \begin{center}
            \includegraphics[width=5.5cm]{severitypredictionwithsex}

        \end{center}

 \end{minipage}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Data analysis}

\underline{Conclusion}:
The prediction is very bad on qualitative features. We notice that the $R^2$ is closed to 0 and it's mostly the same with the others qualitative features. With this dataset, the OLS model is not efficient for qualitative features.

\begin{exampleblock}{Prediction of a quantitative feature}
Predict the number of accidents with the date (day, month and year) that is an ordinal feature with periodic component. Results are also very bad.

\end{exampleblock}

\begin{minipage}[c]{.36\linewidth}
     \begin{center}
             \includegraphics[width=4cm]{stat_model_number1}
         \end{center}
   \end{minipage} \hfill
   \begin{minipage}[c]{.55\linewidth}
    \begin{center}
            \includegraphics[width=5.5cm]{accidentprediction}

        \end{center}

 \end{minipage}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Data analysis}

\begin{exampleblock}{Same thing on the second dataset}
Prediction of the number of bicycles in a day with the date and the total number of bicycles. We introduce also periodic components.
\end{exampleblock}


\begin{minipage}[c]{.36\linewidth}
     \begin{center}
             \includegraphics[width=4cm]{stat_model_albert}
         \end{center}
   \end{minipage} \hfill
   \begin{minipage}[c]{.55\linewidth}
    \begin{center}
            \includegraphics[width=5.5cm]{accidentpredictionalbert1}

        \end{center}

 \end{minipage}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Singular Value Decomposition}
\label{sec:conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{SVD} % XXX TODO give a name

\underline{Reminder}: Let $\Sigma\in\mathbb{R}^{p \times p}$, if $\Sigma^{\top}=\Sigma$ then $\Sigma$ is diagonalizable.

\vspace{0.5cm}

\mytheorem{Theorem}{
For all matrix $M\in\mathbb{R}^{m_1 \times m_2}$ of  ${\rm rank} (r)$, there exist two orthogonal matrix $U\in \mathbb{R}^{m_1 \times r}$ and $V\in\mathbb{R}^{m_2 \times r}$ such that :
\begin{center}
    $M=U \diag(s_{1},\dots, s_{r})U^{\top}$
\end{center}
where $s_{1}\ge s_{2} \ge \dots \ge s_{r} \ge 0$ are the singular values of M.
}

\vspace{0.5cm}

\rem $M=\sum_{j=1}^r s_{j}u_{j}v_{j}^{\top}$ with : $U=[u_{1},\dots,u_{r}]$
et $V=[v_{1} \dots v_{r}]$
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Pseudo-inverse} % XXX TODO give a name

\mytheorem{Definition}{
For $M\in\mathbb{R}^{m_1 \times m_2}$, a pseudoinverse of $M$ is defined as a matrix $M^{+}$ satisfying :
\begin{equation*}
    M^{+}
    = V \diag\left(\frac{1}{s_{1}} \dots \frac{1}{s_{r}}\right)U^{\top}
    = \sum_{j=1}^r \frac{1}{s_{j}}v_{j}u_{j}^{\top}
\end{equation*}
}

\vspace{0.5cm}

\rem If $M$ is invertible, its pseudoinverse is its inverse. That is, $A^{+}=A^{-1}$
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Bibliography}
[1] Joseph Salmon, \textit{Modéle linéaire avancé : introduction}, 2019, \url{http://josephsalmon.eu/enseignement/Montpellier/HMMA307/Introduction.pdf}.
\newline

[2] Francois Portier and Anne Sabourin, \textit{Lecture notes on ordinary least squares}, 2019, \url{https://perso.telecom-paristech.fr/sabourin/mdi720/main.pdf}
\newline
\newline
[3] \textit{Ordinary least squares}, 2020,
\url{https://en.wikipedia.org/wiki/Ordinary_least_squares}.
\newline
\newline
[4] \textit{Singular value decomposition}, 2020,
\url{https://en.wikipedia.org/wiki/Singular_value_decomposition}.

\printbibliography
\end{frame}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}